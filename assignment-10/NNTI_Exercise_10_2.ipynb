{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Name:   \n",
    "Matrikelnummer:  \n",
    "Email:   \n",
    "   \n",
    "Name:   \n",
    "Matrikelnummer:   \n",
    "Email:\n",
    "\n",
    "Name:    \n",
    "Matrikelnummer:    \n",
    "Email:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "## Important: We recommend that you use GPU(s) for these tasks. Google Colab is one such place where you could use GPU(s) for free for a certain duration.\n",
    "\n",
    "# 10.2.1 Design your RNN (4 points)\n",
    "\n",
    "Please create a ```solution.py``` file where you define the following:\n",
    "\n",
    "\n",
    "1. A ```function``` where you use pytorch's Dataset and Dataloader class, and it should return you the desired split for the dataset. The function should have ```split``` as one of its argument and the call to Dataset class should respect this argument. You will manually need to download the dataset first. The desired role of function is as follows:\n",
    "    - Use the ```Large Movie Review Dataset``` dataset. [Link](https://ai.stanford.edu/~amaas/data/sentiment)\n",
    "    - Create Dataset object for different splits\n",
    "    - Computers don't work with natural language, so we have to convert it to some sort of numbers. One such idea would be to use GloVe embeddings for the conversion. Depending on how you choose to do this, you might also have to take care of padding. **Note:** We encourage using the 300d GloVe embeddings.\n",
    "    - Returns the Dataloader object for specified split\n",
    "    - **(Optional)** Try one-hot encoding in-place of GloVe to see how big of an improvement GloVe was for embedding space. There are other (possible but not recommended) ways to do embeddings, such as get POS tags for each word or use a dictionary to define polarity for each word.\n",
    "    \n",
    "2. Multiple ```class``` for your implementation of your networks which does the following:\n",
    "    - Define a RNN class with appropriate layers and hyperparameters\n",
    "    - Define a LSTM class with appropriate layers and hyperparameters\n",
    "    - **(Optionally)** Implement Bi-LSTM, Bi-RNN, Bi-GRU and do a comparison with the one-directional implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f9876-0e5b-496d-9f8d-ce3366e0334f",
   "metadata": {},
   "source": [
    "### Train your model(s) by importing your implementation from ```solution.py``` file in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88c7e2-b596-414b-bd2d-1c4a66adc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0a988-4d98-45b3-9529-19f59e96daf1",
   "metadata": {},
   "source": [
    "# 10.2.2 Transformers (3 points)\n",
    "\n",
    "Let's fine-tune a Transformers based Model for the same task. We will use ```RoBERTa``` for the same. Note that for **converting text to some sort of numbers** you will need to use the tokenizer. HuggingFace has a nice introduction for the same [here](https://huggingface.co/course/chapter1/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd9b92-b41b-4305-ae27-50b39ca2f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
