{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Name:   Anh Tuan Tran\n",
    "Matrikelnummer:  7015463\n",
    "Email:   antr00001@stud.uni-saarland.de\n",
    "   \n",
    "Name:   Deborah Dormah Kanubala\n",
    "Matrikelnummer:   7025906\n",
    "Email:  dkanubala@aimsammi.org\n",
    "\n",
    "Name:    Irem Begüm Gündüz\n",
    "Matrikelnummer:     7026821\n",
    "Email: irgu00001@stud.uni-saarland.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56009306-ebf1-4707-a8fb-58bf315689b5",
   "metadata": {},
   "source": [
    "### Note: This assignment will extensively refer to coding exercise in assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 6.2.a Building your own Neural-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Remember to Fix your seeds for pytorch and numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added an `activation` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$ \\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = activation\\_fn(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c) $ \n",
    "\n",
    "$ \\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{$ softmax $}( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network and are free to implement it with your own design choices. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate Leaky_ReLU, Softmax or the forward/backward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66f378",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the Leaky_ReLu and Softmax function as Class and implement a function in each of them to calculate gradients (1 point)\n",
    "Remember that in PyTorch, these are implemented as classes so we also want to have them as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841a7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaky_Relu():\n",
    "    \"\"\"\n",
    "    Recall your implementation of relu function in assignment 4 and try to implement\n",
    "    Leaky_ReLu similarily, but as a class with a function to calculate gradient\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "           \n",
    "    def leaky_relu(self):\n",
    "        relu = np.maximum(0.01*self.x, self.x)\n",
    "        return relu\n",
    "      \n",
    " \n",
    "    def leaky_relu_grad(self):\n",
    "        dx_relu = np.maximum(self.x, 1)\n",
    "        return dx_relu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3a338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Recall your implementation of softmax function in assignment 4 and try to implement\n",
    "    softmax similarily, but as a class with a function to calculate gradient\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def softmax(self):\n",
    "        return np.exp(self.x)/ np.sum(np.exp(self.x), axis = 0)\n",
    "    \n",
    "    def softmax_grad(self):\n",
    "        softmax_val = self.softmax()\n",
    "        return softmax_val*(1-softmax_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dde97f",
   "metadata": {},
   "source": [
    "## ToDo: Calculate the gradient using your implemented functions in their respective classes and validate by manually calculating gradients using a toy value. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76203a80-9b13-4012-879f-10f78bc6dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The leaky relu is 3.63 and the softmax gradient is 0.0\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "\n",
    "## Leaky relu gradients:\n",
    "val = 3.63\n",
    "leaky_relu_val = Leaky_Relu(val)\n",
    "\n",
    "##Softmax gradients\n",
    "softmax_val = Softmax(val)\n",
    "\n",
    "print(\"The leaky relu is {} and the softmax gradient is {}\".format(leaky_relu_val.leaky_relu_grad(), \n",
    "                                                                  softmax_val.softmax_grad()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4f42",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the code from Assignment 4 to include backpropagation in your class without using pytorch. Remember to use your Leaky_ReLu class here as activation function. (1.5 points)\n",
    "#### Feel free to refer to your solutions from Assignment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f78c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear_weights = []\n",
    "\n",
    "        for i in range (hidden_size):\n",
    "            if i == 0:\n",
    "                inp_dim = input_dim\n",
    "                out_dim = hidden_dim\n",
    "            elif i != hidden_size -1:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = hidden_dim\n",
    "            else:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = output_dim\n",
    "            w = np.random.randn (inp_dim, hidden_dim)\n",
    "            bias = np.random.randn (hidden_dim)\n",
    "            self.linear_weights.append ({\n",
    "                'w': w,\n",
    "                'bias': bias,\n",
    "            })\n",
    "            w_2 = np.random.randn (hidden_dim, out_dim)\n",
    "            bias = np.random.randn (out_dim)\n",
    "            self.linear_weights.append ({\n",
    "                'w': w_2,\n",
    "                'bias': bias,\n",
    "            })\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \n",
    "        \"\"\"\n",
    "        res = x\n",
    "        res = np.array(res)\n",
    "        self.forward_z  = {}\n",
    "        self.forward_h = {}\n",
    "        print(self.hidden_size)\n",
    "        for i in range (0,self.hidden_size+1):\n",
    "            print(f\"the iteration number {i}\")\n",
    "            res = res @ self.linear_weights[i]['w'] + self.linear_weights[i]['bias']\n",
    "            self.forward_h[i] = res\n",
    "            res = Leaky_Relu(res)\n",
    "            res = res.leaky_relu()\n",
    "            self.forward_z[i] = res\n",
    "        res = Softmax(res)\n",
    "        res = res.softmax()\n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "  \n",
    "    def backward_prop(self, x, y, res):\n",
    "        lr = 0.0001\n",
    "        loss = ((res - y)**2/len(x))\n",
    "        print(\"the forward\", self.forward_h)\n",
    "        dx_soft_val1 = Softmax(self.forward_h[1])\n",
    "        dx_soft_val1 = dx_soft_val1.softmax_grad()\n",
    "        dw_2 = dx_soft_val1 *  self.forward_z[0]* loss\n",
    "        dx_soft_val2 = Softmax(self.forward_h[1])\n",
    "        dx_soft_val2 = dx_soft_val2.softmax_grad()\n",
    "        dx_leaky = Leaky_Relu(self.forward_h[0])\n",
    "        dx_leaky = dx_leaky.leaky_relu_grad()\n",
    "        dw_1 = dx_soft_val2 *self.linear_weights[1]['w']* dx_leaky*x*loss\n",
    "        self.linear_weights[0]['w'] = self.linear_weights[0]['w'] - lr * dw_1\n",
    "        self.linear_weights[1]['w'] = self.linear_weights[1]['w'] - lr * dw_2\n",
    "        return self.linear_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "09f74378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "the iteration number 0\n",
      "the iteration number 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# forward prop of our network\n",
    "network = FFNetwork(2, 2, 2, 1)\n",
    "res = network.forward([1.,0.])\n",
    "\n",
    "# backward prop of our network\n",
    "network.backward_prop([12], 10, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2.b.2 Training a network for MNIST (1.5 points)\n",
    "\n",
    "Now that we know how to train a Neural network in Pytorch. Let's start training and evaluating our model using a very standard dataset, for now let's use MNIST. Design a network from scracth using PyTorch and include the followings. Remember that we need to use forward-propagation and backprop.\n",
    "- Training Loop\n",
    "- Optimization \n",
    "- Evaluating Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ba3da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "'''\n",
    "LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "CREATE MODEL CLASS WITH FORWARD AND BACKWARD\n",
    "'''\n",
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation to do classification for MNIST dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        super(TorchFFNetwork, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList ([])\n",
    "        for i in range (hidden_size):\n",
    "            if i == 0:\n",
    "                inp_dim = input_dim\n",
    "                out_dim = hidden_dim\n",
    "            elif i != hidden_size -1:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = hidden_dim\n",
    "            else:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = output_dim\n",
    "            self.linear_layers.append (nn.Linear(inp_dim, out_dim, bias=True))\n",
    "        self.softmax = torch.nn.Softmax (dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        ## SOLUTION ##\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \n",
    "        Note: Remember to apply the ReLU and add the bias for each layer\n",
    "        \"\"\"\n",
    "    \n",
    "        res = x.view(x.size(0), -1)\n",
    "        for i in range (self.hidden_size):\n",
    "            res = self.linear_layers[i](res)\n",
    "            res = self.relu(res)\n",
    "\n",
    "        res = self.softmax(res)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52cb8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# forward prop of our network\n",
    "network = TorchFFNetwork(28*28, 2, 10, 2)\n",
    "preds = network.forward(next(iter(train_loader))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "82c47ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, model, loss_fn, optimizer):  \n",
    "    model.train ()\n",
    "    num_epoch = 4\n",
    "    for i_epoch in range (num_epoch):\n",
    "        epoch_losses = []\n",
    "        for batch in  iter(data):\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn (y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append (loss.detach ().numpy ())\n",
    "#         if i_epoch % 100 == 0:\n",
    "        print (\"epoch:\", i_epoch, \"mean epoch loss:\", np.mean (epoch_losses))\n",
    "\n",
    "def evaluate_loop(data, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(data)\n",
    "    test_loss, correct = 0, 0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iter(data):\n",
    "            \n",
    "            X, y = batch\n",
    "            i += int(y.size(0))\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "            \n",
    "    print(\"old correct\", correct)\n",
    "    test_loss /= i\n",
    "    correct /= i\n",
    "    \n",
    "    print(\"new correct\", correct)\n",
    "    print(f\"Test Error : \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ea87e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 mean epoch loss: 2.3018522\n",
      "epoch: 1 mean epoch loss: 2.3018048\n",
      "epoch: 2 mean epoch loss: 2.3017955\n",
      "epoch: 3 mean epoch loss: 2.3017802\n"
     ]
    }
   ],
   "source": [
    "# Create the model from the forward class and pick a learning rate\n",
    "torch.manual_seed(42)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "(inp_dim, hidden_dim, out_dim, hidden_size) = (28*28, 2, 10, 2)\n",
    "model = TorchFFNetwork(inp_dim, hidden_dim, out_dim, hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loop(train_loader, model, loss_fn, optimizer)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9d0b7b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old correct 1135\n",
      "new correct 0.1135\n",
      "Test Error : \n",
      " Accuracy: 11.3%, Avg loss: 0.076877 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation of the model\n",
    "\n",
    "evaluate_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f17a8-4877-4813-b4ad-cc2d41b05be9",
   "metadata": {},
   "source": [
    "### ToDo: Implement functions for Stochastic Gradient Descent and Stochastic Gradient Descent with momentum and plot the difference on how they change the value for gradients. ( 1 + 1 (Bonus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a243e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
