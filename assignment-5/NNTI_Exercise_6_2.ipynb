{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Name:   \n",
    "Matrikelnummer:  \n",
    "Email:   \n",
    "   \n",
    "Name:   \n",
    "Matrikelnummer:   \n",
    "Email:\n",
    "\n",
    "Name:    \n",
    "Matrikelnummer:    \n",
    "Email:    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56009306-ebf1-4707-a8fb-58bf315689b5",
   "metadata": {},
   "source": [
    "### Note: This assignment will extensively refer to coding exercise in assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 6.2.a Building your own Neural-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Remember to Fix your seeds for pytorch and numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added an `activation` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$ \\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = activation\\_fn(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c) $ \n",
    "\n",
    "$ \\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{$ softmax $}( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network and are free to implement it with your own design choices. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate Leaky_ReLU, Softmax or the forward/backward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66f378",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the Leaky_ReLu and Softmax function as Class and implement a function in each of them to calculate gradients (1 point)\n",
    "Remember that in PyTorch, these are implemented as classes so we also want to have them as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_relu(x):\n",
    "    \"\"\"\n",
    "    Recall your implementation of relu function in assignment 4 and try to implement\n",
    "    Leaky_ReLu similarily, but as a class with a function to calculate gradient\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax(x):\n",
    "    \"\"\"\n",
    "    Recall your implementation of softmax function in assignment 4 and try to implement\n",
    "    softmax similarily, but as a class with a function to calculate gradient\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dde97f",
   "metadata": {},
   "source": [
    "## ToDo: Calculate the gradient using your implemented functions in their respective classes and validate by manually calculating gradients using a toy value. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76203a80-9b13-4012-879f-10f78bc6dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4f42",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the code from Assignment 4 to include backpropagation in your class without using pytorch. Remember to use your Leaky_ReLu class here as activation function. (1.5 points)\n",
    "#### Feel free to refer to your solutions from Assignment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2.b.2 Training a network for MNIST (1.5 points)\n",
    "\n",
    "Now that we know how to train a Neural network in Pytorch. Let's start training and evaluating our model using a very standard dataset, for now let's use MNIST. Design a network from scracth using PyTorch and include the followings. Remember that we need to use forward-propagation and backprop.\n",
    "- Training Loop\n",
    "- Optimization \n",
    "- Evaluating Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation to do classification for MNIST dataset.\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f17a8-4877-4813-b4ad-cc2d41b05be9",
   "metadata": {},
   "source": [
    "### ToDo: Implement functions for Stochastic Gradient Descent and Stochastic Gradient Descent with momentum and plot the difference on how they change the value for gradients. ( 1 + 1 (Bonus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99bc82-b090-43a0-91a3-8b2e8352c772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
