{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGcWOyVClKwF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Name:   \n",
    "MatrikelNummer:  \n",
    "Email:   \n",
    "   \n",
    "Name:   \n",
    "MatrikelNummer:   \n",
    "Email:\n",
    "\n",
    "Name:\n",
    "MatrikelNummer:\n",
    "Email:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4WD6qr5aDMG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.3 Model Capacity, Underfitting and Overfitting (2.5 points)\n",
    "Given training dataset $D = \\{x_i,y_i\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{R}$ the ordinary linear least square regression minimizes the cost function\n",
    "$f(x,y;w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\langle w, x_i \\rangle - b )^2$\n",
    "But sometimes this leads to overfitting or ill posed problems hence we add a regularizational term to the objective function. This is called ridge regression.\n",
    "$f(x,y;w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\langle w, x_i \\rangle - b )^2 + \\lambda \\sum_{i=1}^d w_i^2$\n",
    "In practice we artificially add another dimension of 1's to $X$ to absorb the offset term b such that the objective function in matrix vector form becomes\n",
    "$f(x,y;w) = \\frac{1}{n} \\Vert Y - Xw \\Vert^2 + \\lambda \\Vert w \\Vert^2 $ where\n",
    "    $\n",
    "        X = \\begin{bmatrix}\n",
    "    X_{1,1} & \\cdots & X_{1,d} & 1\\\\\n",
    "    X_{2,1}  & \\cdots & X_{2,d} & 1 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    X_{n,1} & \\cdots & X_{n,d} & 1\n",
    "    \\end{bmatrix}\n",
    "    $\n",
    " and $w = \\begin{bmatrix}w_1 & w_2 & ... & w_d & b \\end{bmatrix}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8egGjEc-Y0WQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlY8pe-ri2NE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3a (0.5 + 0.5 point)\n",
    "Implement the fit() function in the ridge_regression class which takes three parameters X, Y and LAMBDA. The fit() function computes the minimizer $w^*$ as derived in 3.2 for the regularized mean squared error objective function. Store the value of $w^*$ in self.w\n",
    "Implement the predict() function that takes a matrix X and returns the predictions of the model on X and since the bias term is absorbed in $w$, please do not forget to add another dimension of 1's to X as described earlier in both the fit() and predict() functions. Assume that fit() is called before using predict(). \n",
    "**Use only basic matrix vector operations from numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bj11gQWTZM39",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ridge_regression:\n",
    "    def __init__(self):\n",
    "        self.w = None # self.w.shape = (d+1,)\n",
    "    \n",
    "    def fit(self, X, Y, LAMBDA = 0.1): \n",
    "        \"\"\"\n",
    "        args: X.shape = (n, d). We account for the bias parameter b in the \n",
    "                design matrix X by artificially adding another dimension\n",
    "                Y.shape = (n, 1)\n",
    "        returns: None\n",
    "        Note that you have to artificially add a dimension of 1 as defined earlier to account for the bias term\n",
    "        \"\"\"\n",
    "        self.w  = None # TODO: IMPLEMENT\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Note that you have to artificially add a dimension of 1 as defined earlier to account for the bias term\n",
    "        \"\"\"\n",
    "        #TODO IMPLEMENT\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy9ZKJ03lXCU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3b (0.5 + 0.5 point)\n",
    "We create a sample regression dataset using scikit-learn make_regression  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression   \n",
    "The dataset has 100 samples which is randomly split into 70 training samples and 30 test samples. In this exercise we assume $x_i \\in \\mathbb{R}^1$  \n",
    "1. Use a scatter plot to visualize your training (X_train, y_train) and test data (X_test, y_test). Train your ridge regression model on the training data (X_train, y_train) and visualize the regression model for different regularisation coefficient values $\\lambda$ = [0.001, 0.01, 0.1, 1, 10, 100, 1000] using a line graph in the same plot. You can do this by using np.linspace and the predict function of your classifier. Use appropriate legend labels. \n",
    "2. For different regularisation coefficient values $\\lambda$ = [0.001, 0.01, 0.1, 1, 10, 100, 1000] find the mean squared error between the predicted values of test dataset X_test and true labels of the test set. Plot a graph with **log scaled** $\\lambda$ values on X-axis and mean squared error on Y-axis. You may use sklearn.metrics.mean_squared_error()\n",
    "\n",
    "How does changing the values of $\\lambda$ affect the error of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnOpeUWvcgMS",
    "outputId": "743c2938-958e-4a76-aedf-fda041eecc9a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1) (100,)\n",
      "(70, 1) (70,) (30, 1) (30,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create a dummy regression dataset with noise\n",
    "X , y = make_regression(n_samples = 100, n_features = 1, noise = 10)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split the dataset into 70 training samples and 30 test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Different regularization coefficient values\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Use the log scaled values of lambda for plotting\n",
    "log_scale = [-3, -2, -1, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "LJl8PctBeG7u",
    "outputId": "84530b19-7dc1-4b37-a780-bc47dbf77143",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Supply your code for 3.3b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlTLjf4IB4hj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3c (0.25 point)\n",
    "We create a sample regression dataset using scikit-learn make_regression  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression   \n",
    "The dataset has 100 training samples and in this exercise we will plot the weights of the model against different values of $\\lambda$ = [0.001, 0.01, 0.1, 1, 10, 100, 1000]. For this exercise we take the training samples $x_i \\in \\mathbb{R}^{10}$ i.e. d = 10. Since the bias term is absorbed in the parameter $w = \\begin{bmatrix}w_1 & w_2 & ... & w_d & b \\end{bmatrix}^T$ the dimension of $w$ is $d+1$. Therefore we will only consider the first $d$ values (d=10) of self.w  \n",
    "  \n",
    "Train the ridge regression model on the training set (X_train, y_train) for different values of $\\lambda$.\n",
    "Plot a graph with **log scaled** lambda values on X-axis and each $w_i$ on the Y-axis on the same plot. Explain what happens to the weight values and model capacity as you increase the regularization coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtCXZ9DRCGkl",
    "outputId": "f846d0c0-209c-4ac0-8851-81f7e36b8020",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , y_train = make_regression(n_samples = 100, n_features = 10, noise = 10)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sz6plY5UygxK",
    "outputId": "b8599a9b-9b94-49ac-b8e2-0737faec76a5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Supply your code for 3.3c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHcycVmWoEAK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3d (0.25 point)\n",
    "Answer the following questions  \n",
    "1. Do you observe overfitting or underfitting for different values of Î» in this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Solution:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NNTI_Ex3_2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}