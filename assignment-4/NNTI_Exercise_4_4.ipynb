{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Name:   Anh Tuan Tran\n",
    "Matrikelnummer:  7015463\n",
    "Email:   antr00001@stud.uni-saarland.de\n",
    "   \n",
    "Name:   Deborah Dormah Kanubala\n",
    "Matrikelnummer:   7025906\n",
    "Email:  dkanubala@aimsammi.org\n",
    "\n",
    "Name:    Irem Begüm Gündüz\n",
    "Matrikelnummer:     7026821\n",
    "Email: irgu00001@stud.uni-saarland.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 4.4.a Building your own feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added a `ReLU` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$ \\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = ReLU(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c) $ \n",
    "\n",
    "$ \\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{$ softmax $}( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate the ReLU, Softmax or the forward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function as defined in the lecture\n",
    "    Input: an array of numbers\n",
    "    Output: ReLU(x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # raise NotImplementedError\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implement the `softmax` function as defined in the lecture\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # raise NotImplementedError\n",
    "    e_x = np.exp (x)\n",
    "    ret = e_x / np.sum (e_x, -1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e87fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        # Initialize each layer as a random matrix of the\n",
    "        # appropriate dimensions\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear_weights = []\n",
    "        for i in range (hidden_size):\n",
    "            if i == 0:\n",
    "                inp_dim = input_dim\n",
    "                out_dim = hidden_dim\n",
    "            elif i != hidden_size -1:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = hidden_dim\n",
    "            else:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = output_dim\n",
    "            w = np.random.randn (inp_dim, out_dim)\n",
    "            bias = np.random.randn (out_dim)\n",
    "            self.linear_weights.append ({\n",
    "                'w': w,\n",
    "                'bias': bias,\n",
    "            })\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \n",
    "        Note: Remember to apply the ReLU and add the bias for each layer\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the network,\n",
    "        # i.e. calculate `y` from an input `x`\n",
    "        # Remember that each layer's output is calculated by\n",
    "        # f^(i) = ReLU(W_i^T * f^(i-1) + b_i)\n",
    "        res = x\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        res = np.array (res)\n",
    "        for i in range (self.hidden_size):\n",
    "            res = res @ self.linear_weights[i]['w'] + self.linear_weights[i]['bias']\n",
    "            res = relu (res)\n",
    "        res = softmax (res)\n",
    "        ## SOLUTION ##\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd64a7",
   "metadata": {},
   "source": [
    "Your implementation needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2109ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97420925, 0.02579075])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# A configuration that reflects the example from the lecture\n",
    "# i.e. our input is of size 2, our intermediate layers are also of size 2,\n",
    "# and we will only have 1 hidden layer.\n",
    "network = FFNetwork(2, 2, 2, 1)\n",
    "network.forward([1.,0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae8166",
   "metadata": {},
   "source": [
    "Disclaimer: Do not expect a correct output at this stage, you are simply building the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d5d26",
   "metadata": {},
   "source": [
    "However, our setup also allows us to create larger networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452ce6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2164466, 0.7835534])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "network = FFNetwork(2, 3, 2, 4)\n",
    "network.forward([1.,0.]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc484337",
   "metadata": {},
   "source": [
    "Some sanity checks:\n",
    "\n",
    "1. You should be seeing the number of units you specified as output units in your output.\n",
    "1. The numbers in your outputs should be in the range $[0,1]$\n",
    "1. The numbers should add up to $1$\n",
    "1. Varying the structure of the network should not break its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72dc37b-6595-47fc-b7bf-6532a2c1c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# TVarying the structure of the network should not break its functionality.\n",
    "\n",
    "for inp_dim in range (2, 5):\n",
    "    for out_dim in range (2, 5):\n",
    "        for hidden_dim in range (2, 5):\n",
    "            for hidden_size in range (2, 5):\n",
    "                network = FFNetwork(inp_dim, hidden_dim, out_dim, hidden_size)\n",
    "                output = network.forward(np.random.randn (inp_dim)) \n",
    "\n",
    "                # You should be seeing the number of units you specified as output units in your output.\n",
    "                assert (len (output) == network.output_dim)\n",
    "                # The numbers in your outputs should be in the range [0,1]\n",
    "                assert (not np.any (output < 0) & np.any (output > 1))\n",
    "                # The numbers should add up to 1\n",
    "                assert (np.abs (np.sum (output) - 1 < 1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba70186",
   "metadata": {},
   "source": [
    "## 4.4.b Implementing a feed-forward network using `torch`\n",
    "\n",
    "### 4.4.b.1 Creating the network (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167133e",
   "metadata": {},
   "source": [
    "For this we will be using the `nn` module of `torch`, which contains modules representing types of layers. In your case, the specific relevant module would be that of a *fully connected linear layer*.\n",
    "\n",
    "We will also be using the `nn.functional` module to take advantage of the built in functions for ReLU and Softmax. In this exercise, you are allowed to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3298c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed85010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A `torch` version of the network implemented for 4.3.b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        super(TorchFFNetwork, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList ([])\n",
    "        for i in range (hidden_size):\n",
    "            if i == 0:\n",
    "                inp_dim = input_dim\n",
    "                out_dim = hidden_dim\n",
    "            elif i != hidden_size -1:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = hidden_dim\n",
    "            else:\n",
    "                inp_dim = hidden_dim\n",
    "                out_dim = output_dim\n",
    "            self.linear_layers.append (nn.Linear (inp_dim, out_dim, bias=True))\n",
    "        self.softmax = nn.Softmax (dim=1)\n",
    "        self.relu = nn.ReLU ()\n",
    "        ## SOLUTION ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## SOLUTION ##\n",
    "        if (len (x.shape) == 1):\n",
    "            # Extending the first dimension for batch \n",
    "            res = x[None] \n",
    "        else:\n",
    "            res = x\n",
    "        for i in range (self.hidden_size):\n",
    "            res = self.linear_layers [i] (res)\n",
    "            res = self.relu (res)\n",
    "        res = self.softmax (res)\n",
    "        \n",
    "        if (len (x.shape) == 1):\n",
    "            return res [0]\n",
    "        else:\n",
    "            return res\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871745",
   "metadata": {},
   "source": [
    "Your implementation, once more, needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1c4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_network = TorchFFNetwork(2, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2cfabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2167, 0.5665, 0.2167])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch_network(torch.tensor([1.,0.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfc177",
   "metadata": {},
   "source": [
    "Note that the `forward` method is automatically called when you call your network object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {},
   "source": [
    "### 4.4.b.2 Training your network (1 point)\n",
    "\n",
    "Even though we have not covered how training actually works, we will proceed with the training of the a neural network as a blackbox procedure and we will later on learn the internals of the training process (and even implement them ourselves!).\n",
    "\n",
    "For now, train a neural network (the one you created above) to learn the XOR operation. You are to create a neural network with the appropriate number of input variables, an intermediate hidden layer with 2 units and an output layer with 2 units.\n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimization-loop). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- You are to train the network until the network learns the operation. Remember to set your random seeds so the results are reproducible.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD. It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training X, where each instance includes an x1 and an x2, (where the operation is defined as x1 XOR x2)\n",
    "training_x = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "# We have only covered softmax in the lecture, so we format the output as follows:\n",
    "training_y = [[1,0], [0,1], [0,1], [1,0]]\n",
    "\n",
    "# The Y is formatted such that the its first element corresponds to the probability of the XOR resulting in a 0\n",
    "# and the second element to the probability of the XOR resulting in a 1\n",
    "\n",
    "################################################################\n",
    "# TODO: Adapt the training set so it can be used with `pytorch`\n",
    "################################################################\n",
    "\n",
    "\n",
    "training_x = torch.tensor (training_x, dtype=torch.float32)\n",
    "training_y = torch.tensor (training_y, dtype=torch.float32)\n",
    "training_data = (training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8697ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model from the previous class and pick a learning rate\n",
    "torch.manual_seed(42)\n",
    "(inp_dim, hidden_dim, out_dim, hidden_size) = (2, 10, 2, 3)\n",
    "model = TorchFFNetwork(inp_dim, hidden_dim, out_dim, hidden_size)\n",
    "model.train ()\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65516831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, model, loss_fn, optimizer):\n",
    "    # TODO: Implement\n",
    "    \n",
    "    num_epoch = 5000\n",
    "    training_x, training_y = data\n",
    "    for i_epoch in range (num_epoch):\n",
    "        epoch_losses = []\n",
    "        for x, y in zip (training_x, training_y):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model (x)\n",
    "            loss = loss_fn (y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append (loss.detach ().numpy ())\n",
    "        if i_epoch % 100 == 0:\n",
    "            print (\"epoch:\", i_epoch, \"mean epoch loss:\", np.mean (epoch_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58208004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 mean epoch loss: 0.25630927\n",
      "epoch: 100 mean epoch loss: 0.0012917169\n",
      "epoch: 200 mean epoch loss: 0.00022464931\n",
      "epoch: 300 mean epoch loss: 8.97075e-05\n",
      "epoch: 400 mean epoch loss: 4.6921836e-05\n",
      "epoch: 500 mean epoch loss: 2.8103903e-05\n",
      "epoch: 600 mean epoch loss: 1.8245566e-05\n",
      "epoch: 700 mean epoch loss: 1.2460289e-05\n",
      "epoch: 800 mean epoch loss: 8.836173e-06\n",
      "epoch: 900 mean epoch loss: 6.4254327e-06\n",
      "epoch: 1000 mean epoch loss: 4.7666604e-06\n",
      "epoch: 1100 mean epoch loss: 3.5876328e-06\n",
      "epoch: 1200 mean epoch loss: 2.7335243e-06\n",
      "epoch: 1300 mean epoch loss: 2.1008789e-06\n",
      "epoch: 1400 mean epoch loss: 1.625893e-06\n",
      "epoch: 1500 mean epoch loss: 1.2671265e-06\n",
      "epoch: 1600 mean epoch loss: 9.919838e-07\n",
      "epoch: 1700 mean epoch loss: 7.792564e-07\n",
      "epoch: 1800 mean epoch loss: 6.1426437e-07\n",
      "epoch: 1900 mean epoch loss: 4.855406e-07\n",
      "epoch: 2000 mean epoch loss: 3.8477003e-07\n",
      "epoch: 2100 mean epoch loss: 3.0529534e-07\n",
      "epoch: 2200 mean epoch loss: 2.427858e-07\n",
      "epoch: 2300 mean epoch loss: 1.9324571e-07\n",
      "epoch: 2400 mean epoch loss: 1.5412775e-07\n",
      "epoch: 2500 mean epoch loss: 1.2296728e-07\n",
      "epoch: 2600 mean epoch loss: 9.820745e-08\n",
      "epoch: 2700 mean epoch loss: 7.853431e-08\n",
      "epoch: 2800 mean epoch loss: 6.2833735e-08\n",
      "epoch: 2900 mean epoch loss: 5.029884e-08\n",
      "epoch: 3000 mean epoch loss: 4.0300034e-08\n",
      "epoch: 3100 mean epoch loss: 3.2279708e-08\n",
      "epoch: 3200 mean epoch loss: 2.5880318e-08\n",
      "epoch: 3300 mean epoch loss: 2.0759021e-08\n",
      "epoch: 3400 mean epoch loss: 1.6666926e-08\n",
      "epoch: 3500 mean epoch loss: 1.338117e-08\n",
      "epoch: 3600 mean epoch loss: 1.07503695e-08\n",
      "epoch: 3700 mean epoch loss: 8.6383425e-09\n",
      "epoch: 3800 mean epoch loss: 6.938916e-09\n",
      "epoch: 3900 mean epoch loss: 5.5828693e-09\n",
      "epoch: 4000 mean epoch loss: 4.48824e-09\n",
      "epoch: 4100 mean epoch loss: 3.6094268e-09\n",
      "epoch: 4200 mean epoch loss: 2.9076896e-09\n",
      "epoch: 4300 mean epoch loss: 2.339825e-09\n",
      "epoch: 4400 mean epoch loss: 1.884699e-09\n",
      "epoch: 4500 mean epoch loss: 1.5181374e-09\n",
      "epoch: 4600 mean epoch loss: 1.222052e-09\n",
      "epoch: 4700 mean epoch loss: 9.867619e-10\n",
      "epoch: 4800 mean epoch loss: 7.958616e-10\n",
      "epoch: 4900 mean epoch loss: 6.42436e-10\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run training\n",
    "train_loop (training_data, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6081e1f-d663-45f3-88fe-c3a9a79c7724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([0., 0.])\n",
      "output tensor([9.9997e-01, 2.7327e-05])\n",
      "gt output tensor([1., 0.])\n",
      "\n",
      "input tensor([0., 1.])\n",
      "output tensor([2.6398e-05, 9.9997e-01])\n",
      "gt output tensor([0., 1.])\n",
      "\n",
      "input tensor([1., 0.])\n",
      "output tensor([1.8236e-05, 9.9998e-01])\n",
      "gt output tensor([0., 1.])\n",
      "\n",
      "input tensor([1., 1.])\n",
      "output tensor([9.9998e-01, 1.7359e-05])\n",
      "gt output tensor([1., 0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "with torch.no_grad ():\n",
    "    for x, y in zip (training_x, training_y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model (x)\n",
    "        print ('input', x)\n",
    "        print ('output', y_pred)\n",
    "        print ('gt output', y)\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562b8f0-bc02-4aa6-9630-f206fcffaa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ce260-70cd-4544-881a-95b854ce52d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "610c699f0cd8c4f129acd9140687fff6866bed0eb8e82f249fc8848b827b628c"
  },
  "kernelspec": {
   "display_name": "plenoxels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c8c1ef1c72bc4e3d3e99c1985efd4140ae9806de4257400960ff7c06a308fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
